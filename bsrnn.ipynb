{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a4d6f-e587-4f57-8a1d-1c54412d69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import IPython.display as idp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee4ca1-ac3e-40d7-845a-23f6c4f625ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize=True\n",
    "# Feature gate: Overtones Splits\n",
    "fg_generic_bands=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e425c21-aa1a-419c-904e-c68e9a683582",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Input and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cbc51-70f7-41c4-be4c-cb6a7c5debd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio\n",
    "\n",
    "sample_rate = 44100 # \n",
    "\n",
    "def show_idp_audio(waveform):\n",
    "    n = 14\n",
    "    return idp.display(idp.Audio(waveform[(3 * n) * sample_rate:(3 * (n + 1)) * sample_rate], rate=sample_rate))\n",
    "\n",
    "def load_audio(path, visualize=False):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    # Convert everthing to mono channel for simplicity\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "        # waveform is now a vector \n",
    "    # Resample everything to 44.1khz for simplicity\n",
    "    resampler = T.Resample(sr, sample_rate, dtype=waveform.dtype)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    if visualize:\n",
    "        # samplerate = 1/t\n",
    "        # display the first 3 seconds\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "if visualize:\n",
    "    sample_waveform = load_audio(\"mixture.wav\", visualize=True)\n",
    "\n",
    "    sample_waveform.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db49d1-7007-4b1f-a9d0-e4a68dfb773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_normalize(waveform, target_rms):\n",
    "    current_rms = torch.sqrt(torch.mean(waveform**2))\n",
    "    gain_factor = target_rms / (current_rms + 1e-10)\n",
    "    normalized_waveform = waveform * gain_factor\n",
    "    return normalized_waveform, gain_factor\n",
    "\n",
    "def rms_denormalize(normalized_waveform, gain_factor):\n",
    "    inverse_gain = 1 / gain_factor\n",
    "    reversed_waveform = normalized_waveform * inverse_gain\n",
    "    return reversed_waveform\n",
    "\n",
    "def peak_normalize(waveform, target_peak):\n",
    "    peak_value = torch.max(torch.abs(waveform))\n",
    "    peak_gain_factor = target_peak / (peak_value + 1e-10)\n",
    "    normalized_waveform = waveform * peak_gain_factor\n",
    "    return normalized_waveform, peak_gain_factor\n",
    "\n",
    "def peak_denormalize(normalized_waveform, peak_gain_factor):\n",
    "    inverse_peak_gain = 1 / peak_gain_factor\n",
    "    reversed_waveform = normalized_waveform * inverse_peak_gain\n",
    "    return reversed_waveform\n",
    "\n",
    "def inspect_waveform(waveform):\n",
    "    transform = T.Loudness(sample_rate)\n",
    "    return f\"LKFS:{transform(waveform.unsqueeze(0))} max: {waveform.max()} min: {waveform.min()} avg: {waveform.mean()}\"\n",
    "\n",
    "def normalize_waveform(waveform, visualize=False):\n",
    "    \"\"\" rms -> peak \"\"\"\n",
    "    # target rms can be anything. the important part here\n",
    "    # is to be constant for all kind of songs\n",
    "\n",
    "    if visualize:\n",
    "        print(\"original: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "\n",
    "    normalized_waveform, gain_factor = rms_normalize(waveform, target_rms=0.1)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"rms_normalize: \" + inspect_waveform(normalized_waveform))\n",
    "        show_idp_audio(normalized_waveform)\n",
    "\n",
    "    # setting target peak to 1.0 forces the values between -1.0 < y < 1.0\n",
    "    normalized_waveform, peak_gain_factor = peak_normalize(normalized_waveform, target_peak=0.1)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"peak_normalize: \" + inspect_waveform(normalized_waveform))\n",
    "        show_idp_audio(normalized_waveform)\n",
    "    \n",
    "    return normalized_waveform, gain_factor, peak_gain_factor\n",
    "\n",
    "def de_normalize_waveform(waveform, gain_factor, peak_gain_factor, visualize=False):\n",
    "    if visualize:\n",
    "        print(\"de_normalize_waveform: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    waveform = peak_denormalize(waveform, peak_gain_factor)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"peak_denormalize: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    waveform = rms_denormalize(waveform, gain_factor)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"rms_denormalize: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "if visualize:\n",
    "    normal_waveform, gain_factor, peak_gain_factor = normalize_waveform(sample_waveform, visualize=True)\n",
    "    _ = de_normalize_waveform(normal_waveform, gain_factor, peak_gain_factor, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7787ec7-063b-47f3-9549-578fc962cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_in_seconds = 1\n",
    "chunk_size = chunk_size_in_seconds * sample_rate\n",
    "\n",
    "def split(waveform, visualize=False):\n",
    "    # we have a vector by length n and we want to split it to even chunks by length of\n",
    "    # chunk_size\n",
    "    padding_length = (chunk_size - waveform.shape[0] % chunk_size) % chunk_size\n",
    "    waveform = nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "    # -1 means automatically infer based on other dims\n",
    "    chunked_waveform = waveform.view(-1, chunk_size)\n",
    "    \n",
    "    if visualize:\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(16, 4))\n",
    "        subfigs = fig.subfigures(2, 1).flat\n",
    "        \n",
    "        # first 3 chunk_size of waveform\n",
    "        w = waveform[:3 * chunk_size].detach().numpy()\n",
    "        ylim = [w.max() * 1.1, w.min() * 1.1]\n",
    "        def time_axis(start, duration):\n",
    "            return torch.arange(start * sample_rate, (duration + start) * sample_rate) / sample_rate\n",
    "        axes = subfigs[0].subplots(1, 1)\n",
    "        axes.plot(time_axis(0, 3), w, linewidth=0.3)\n",
    "        axes.set_xlabel(\"time [s] for first 3 seconds\")\n",
    "        axes.set_ylim(ylim)\n",
    "        \n",
    "        # first 4 chunks + last chunk\n",
    "        axes = subfigs[1].subplots(1, 5)\n",
    "        for i, chunk in enumerate([0, 1, 3, 4, chunked_waveform.shape[0] - 1]): \n",
    "            axes[i].plot(time_axis(0, chunk_size_in_seconds), chunked_waveform[chunk], linewidth=0.3)\n",
    "            axes[i].set_title(f\"chunk {chunk}\")\n",
    "            axes[i].set_ylim(ylim)\n",
    "        \n",
    "    return chunked_waveform, padding_length\n",
    "\n",
    "def merge(chunks, padding_length):\n",
    "    merged_waveform = torch.cat([torch.flatten(x) for x in chunks])\n",
    "    return merged_waveform[:-padding_length]\n",
    "\n",
    "if visualize:\n",
    "    sample_waveform_chunks, padding_length = split(normal_waveform, visualize=True)\n",
    "\n",
    "    assert sample_waveform_chunks.shape[1] == chunk_size\n",
    "\n",
    "    sample_merged = merge(sample_waveform_chunks, padding_length)\n",
    "\n",
    "    assert sample_merged.shape == normal_waveform.shape\n",
    "    assert torch.all(sample_merged == normal_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f9132-958f-4704-8ad9-3a22c4138b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "win_length = n_fft\n",
    "hop_length = win_length // 4\n",
    "\n",
    "\n",
    "def visualize_spectogram(chunk, chunk_stft, title='Spectogram'):\n",
    "    import librosa\n",
    "    fig, axis = plt.subplots(2, 1, figsize=(16, 5))\n",
    "    noverlap = win_length - hop_length\n",
    "    axis[0].imshow(librosa.power_to_db(chunk_stft.abs().detach().numpy() ** 2), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axis[0].set_yscale(\"symlog\")\n",
    "    axis[0].set_title(title)\n",
    "    if chunk is not None:\n",
    "        axis[1].plot(chunk, linewidth=0.5)\n",
    "        axis[1].grid(True)\n",
    "        axis[1].set_xlim([0, len(chunk)])\n",
    "\n",
    "def to_spectogram():\n",
    "    transform_spectogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length,\\\n",
    "                                         window_fn=torch.hamming_window, power=None)\n",
    "    def inner(chunk, visualize=False, title=''):\n",
    "        chunk_stft = transform_spectogram(chunk)\n",
    "        if visualize:\n",
    "            visualize_spectogram(chunk, chunk_stft, title)\n",
    "\n",
    "        return chunk_stft\n",
    "    return inner\n",
    "\n",
    "if visualize:\n",
    "    print(sample_waveform_chunks[1].shape)\n",
    "    chunk_stft = to_spectogram()(sample_waveform_chunks[1], visualize=True)\n",
    "    chunk_stft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e3f90-057c-48b1-8340-6b158236836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vis:\n",
    "    def visualize(self, input):\n",
    "        from torchview import draw_graph\n",
    "        y = self(input)\n",
    "        x = draw_graph(self, input_data=input, device='meta', roll=True)\n",
    "        print(f\"--{input.shape}-->f(x)--{y.shape}-->\")\n",
    "        file = x.visual_graph.render(self._get_name())\n",
    "        display(idp.FileLink(\"./\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ea2b1-dcce-48d8-a01e-5f8e9919bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "def get_microtone_name(semitones_from_A4, divisions_per_octave):\n",
    "    notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    microtone_index = int((semitones_from_A4 + 9) * divisions_per_octave / 12) % divisions_per_octave\n",
    "    octave = int((semitones_from_A4 + 9) // 12)\n",
    "    note_index = microtone_index // (divisions_per_octave // 12)\n",
    "    return notes[note_index] + str(octave)\n",
    "\n",
    "def microtonal_notes(divisions_per_octave):\n",
    "    A4_freq = 440.0\n",
    "    min_freq = 20.0\n",
    "    max_freq = 20000.0\n",
    "    notes = []\n",
    "    freqs = []\n",
    "    current_freq = min_freq\n",
    "    while current_freq <= max_freq:\n",
    "        semitones_from_A4 = 12 * np.log2(current_freq / A4_freq)\n",
    "        nearest_microtone = round(semitones_from_A4 * divisions_per_octave / 12)\n",
    "        nearest_freq = A4_freq * (2 ** (nearest_microtone / divisions_per_octave))\n",
    "\n",
    "        if nearest_microtone % (divisions_per_octave // 12) != 0:\n",
    "            note_name = get_microtone_name(nearest_microtone / (divisions_per_octave / 12), divisions_per_octave)\n",
    "            notes.append(note_name)\n",
    "            freqs.append(nearest_freq)\n",
    "\n",
    "        current_freq = A4_freq * (2 ** ((nearest_microtone + 1) / divisions_per_octave))\n",
    "    return (notes, freqs)\n",
    "\n",
    "def plot_custom_labeled_attention_distribution(splits):\n",
    "    start_freq = 0\n",
    "    events = []\n",
    "    for end_freq, step_size in splits:\n",
    "        while start_freq < end_freq:\n",
    "            start_freq += step_size\n",
    "            events.append(start_freq)\n",
    "\n",
    "    notes, positions = microtonal_notes(24)\n",
    "    print(len(events))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yticks([])\n",
    "    plt.xticks(evenly_skip_elements(positions, 39), evenly_skip_elements(notes, 39), rotation=90, fontsize=20)\n",
    "    plt.eventplot(events, orientation='horizontal', colors='b')\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "if visualize:\n",
    "    plot_custom_labeled_attention_distribution(splits_v7)\n",
    "\n",
    "def evenly_skip_elements(input_list, k):\n",
    "    \"\"\"\n",
    "    Reduce the size of the input list to 'k' by skipping elements evenly.\n",
    "\n",
    "    :param input_list: The original list.\n",
    "    :param k: The desired size of the new list, must be smaller or equal to the size of input_list.\n",
    "    :return: A new list of size 'k'.\n",
    "    \"\"\"\n",
    "    if k > len(input_list):\n",
    "        raise ValueError(\"k must be smaller or equal to the size of the input list\")\n",
    "\n",
    "    # Calculate the step for even skipping\n",
    "    step = len(input_list) / k\n",
    "\n",
    "    # Generate the new list\n",
    "    new_list = [input_list[int(i * step)] for i in range(k)]\n",
    "\n",
    "    return new_list\n",
    "\n",
    "def create_evenly_distributed_splits(num_splits):\n",
    "    _, freqs = microtonal_notes(24)\n",
    "    splits = []\n",
    "    last_freq = 0\n",
    "    for freq in evenly_skip_elements(freqs, num_splits):\n",
    "        splits.append((freq, freq - last_freq))\n",
    "        last_freq = freq\n",
    "    return splits\n",
    "    \n",
    "\n",
    "splits_generic = create_evenly_distributed_splits(41)\n",
    "if visualize:\n",
    "    plot_custom_labeled_attention_distribution(splits_generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bc7a8-ce1f-49c8-84a8-6fa2ea963b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    data1 = {\n",
    "        'Step': [1, 2, 3, 4, 5],\n",
    "        'Value': [3, 7, 5, 8, 6]\n",
    "    }\n",
    "    data2 = {\n",
    "        'Step': [1, 2, 3, 4, 5],\n",
    "        'Value': [4, 1, 3, 6, 2]\n",
    "    }\n",
    "\n",
    "    df1 = pd.read_csv(\"baseline-drums-to-bass-second-try-2023-12-26 16_31_35.352266_tb.csv\")\n",
    "    df2 = pd.read_csv(\"generic-splits-drums-to-bass-second-try-2023-12-26 20_50_55.859633_tb.csv\")\n",
    "\n",
    "    plt.plot(df1['Step'], df1['Value'], label='v7 Splits')\n",
    "    plt.plot(df2['Step'], df2['Value'], label='Generic splits')\n",
    "    plt.xlim([0, 160])\n",
    "    plt.title('CSV Data Comparison')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('uSDR')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf8d8e-eda8-4354-9256-df3786f80e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers are exctracted from the paper\n",
    "splits_v7 = [\n",
    "   # below 1kh, bandwidth 100hz\n",
    "   (1000, 100),\n",
    "   # above 1kh and below 4khz, bandwidth 250hz\n",
    "   (4000, 250),\n",
    "   (8000, 500),\n",
    "   (16000, 1000),\n",
    "   (20000, 2000),\n",
    "]\n",
    "\n",
    "temporal_dim = int(np.ceil(chunk_size / T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length).hop_length))\n",
    "feature_dim = 128\n",
    "\n",
    "# Module 1\n",
    "class BandSplit(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self, splits=splits_generic if fg_generic_bands else splits_v7, fully_connected_out=feature_dim):\n",
    "        super(BandSplit, self).__init__()\n",
    "        \n",
    "        \n",
    "        #### Make splits\n",
    "        # convert fft to freq\n",
    "        freqs = sample_rate * torch.fft.fftfreq(n_fft)[:n_fft // 2 + 1]\n",
    "        freqs[-1] = sample_rate // 2\n",
    "        indices = []\n",
    "        start_freq, start_index = 0, 0\n",
    "        for end_freq, step in splits:\n",
    "            bands = torch.arange(start_freq + step, end_freq + step, step)\n",
    "            start_freq = end_freq\n",
    "            for band in bands:\n",
    "                end_index = freqs[freqs < band].shape[0]\n",
    "                if end_index != start_index or not fg_generic_bands:\n",
    "                    indices.append((start_index, end_index))\n",
    "                start_index = end_index\n",
    "        indices.append((start_index, freqs.shape[0]))\n",
    "        self.band_indices = indices\n",
    "        print(self.band_indices)\n",
    "        self.fully_connected_out = fully_connected_out\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            # * 2 is for added dim of view_as_real\n",
    "            nn.LayerNorm([(band_end - band_start) * 2, temporal_dim])\n",
    "            for band_start, band_end in self.band_indices\n",
    "        ])\n",
    "        \n",
    "        self.layer_fcs =  nn.ModuleList([\n",
    "            # * 2 is for added dim of view_as_real\n",
    "            nn.Linear((band_end - band_start) * 2, self.fully_connected_out)\n",
    "            for band_start, band_end in self.band_indices\n",
    "        ])\n",
    "\n",
    "    def forward(self, chunk_ftt):\n",
    "        batch_size = chunk_ftt.size(0)\n",
    "        stack = []\n",
    "        # TODO: can i vectorize this loop?\n",
    "        for i, (band_start, band_end) in enumerate(self.band_indices):\n",
    "            band = chunk_ftt[:, band_start:band_end, :]\n",
    "            # band is shape of (B, F, T)\n",
    "            band = torch.view_as_real(band) # (B, F, T, 2)\n",
    "            # convert to (B, 2, F, T) to be able to feed it to the norm\n",
    "            band = band.permute(0, 3, 1, 2)\n",
    "            \n",
    "            # norm is (..., F, T) and fc is (Fxfully_connected_out)\n",
    "            # we should make norm (..., T, F) in order to feed it to the fc\n",
    "            band = band.reshape(batch_size, -1, band.size(-1)) # -1 = T\n",
    "            norm = self.layer_norms[i](band)\n",
    "            \n",
    "            norm = norm.transpose(-1, -2).contiguous()\n",
    "            fc_y = self.layer_fcs[i](norm)\n",
    "            \n",
    "            stack.append(fc_y)\n",
    "        return torch.stack(stack, dim=1)\n",
    "\n",
    "if visualize:\n",
    "    bandsplit_layer = BandSplit()\n",
    "    bandsplit_y = bandsplit_layer(chunk_stft.unsqueeze(0))\n",
    "    bandsplit_layer.visualize(chunk_stft.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7dfeea-f1c3-41a7-a4ff-06b0f0e39e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_dim_size = input_dim_size\n",
    "        # paper specified group norm\n",
    "        self.norm = nn.ModuleList([nn.GroupNorm(self.input_dim_size, self.input_dim_size) for _ in range(2)])\n",
    "        self.blstm = nn.ModuleList([nn.LSTM(self.input_dim_size, self.input_dim_size, bidirectional=True, batch_first=True) for _ in range(2)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(self.input_dim_size * 2, self.input_dim_size) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is b, bands(K), temporal_dim(t), input_dim_size\n",
    "        \n",
    "        \n",
    "        # First loops converts the shape to [B, T, K, N]\n",
    "        # and the second loop converts it back to [B, K, T, N]\n",
    "        for i in range(2):\n",
    "            B, K, T, N = x.shape\n",
    "            out = x.view(B * K, T, N)\n",
    "            out = self.norm[i](out.transpose(-1, -2)).transpose(-1, -2)\n",
    "            out = self.blstm[i](out)[0]\n",
    "            out = self.fc[i](out)\n",
    "            x = out.view(B, K, T, N) + x\n",
    "            x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        return x\n",
    "\n",
    "num_blstm_layers=24\n",
    "\n",
    "class BandSequence(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self, input_dim_size, num_layers=num_blstm_layers):\n",
    "        super(BandSequence, self).__init__()\n",
    "        self.rnns = nn.Sequential(*[RNN(input_dim_size=input_dim_size) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (bands, temporal_dim, fc_out)\n",
    "        return self.rnns(x)\n",
    "\n",
    "if visualize:\n",
    "    bandsequence_layer = BandSequence(input_dim_size=bandsplit_layer.fully_connected_out)\n",
    "    bandsequence_y = bandsequence_layer(bandsplit_y)\n",
    "    bandsequence_layer.visualize(bandsplit_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe8f3d-b9ff-4e24-b532-bbbdfd45b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "mlp_dim = 512\n",
    "\n",
    "class MaskEstimation(nn.Module, Vis):\n",
    "    def __init__(self, band_indices, fully_connected_out):\n",
    "        super(MaskEstimation, self).__init__()\n",
    "        \n",
    "        max_indice_diff = max([e - s for s, e in band_indices])\n",
    "        num_hiddens = lambda e, s: 3 * (max_indice_diff - (e - s) + 1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm([temporal_dim, fully_connected_out]),\n",
    "                nn.Linear(fully_connected_out, mlp_dim),\n",
    "                nn.Tanh(),\n",
    "                # double the output dim to use in GLU\n",
    "                # the extra *2 is for returning as complex\n",
    "                nn.Linear(mlp_dim, (e - s) * 2 * 2),\n",
    "                nn.GLU()\n",
    "            )\n",
    "            for s, e in band_indices\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (b, k, temporal_dim, fc_out)\n",
    "        parts = []\n",
    "        for i in range(x.shape[1]):\n",
    "            y = self.layers[i](x[:, i]).contiguous()\n",
    "            B, T, F = y.shape\n",
    "            y = y.permute(0, 2, 1).contiguous() # B F T\n",
    "            # basically halve the freq dim and use it for phasee\n",
    "            y = y.view(B, 2, F // 2, T) # (B, 2, F, T)\n",
    "            y = y.permute(0, 2, 3, 1) # (B, F, T, 2)\n",
    "            y = torch.view_as_complex(y.contiguous())\n",
    "            \n",
    "            parts.append(y)\n",
    "        \n",
    "        # (b, f, t)\n",
    "        return torch.cat(parts, dim=-2)\n",
    "\n",
    "    \n",
    "if visualize:   \n",
    "    mask_layer = MaskEstimation(band_indices=bandsplit_layer.band_indices, fully_connected_out=bandsplit_layer.fully_connected_out)\n",
    "    mask_y = mask_layer(bandsequence_y)\n",
    "    mask_layer.visualize(bandsequence_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d0886-e194-4a72-8a81-b27554be18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSRNN(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BSRNN, self).__init__()\n",
    "        \n",
    "        self.split = BandSplit()\n",
    "        self.sequence = BandSequence(input_dim_size=self.split.fully_connected_out)\n",
    "        self.mask = MaskEstimation(band_indices=self.split.band_indices, fully_connected_out=self.split.fully_connected_out)\n",
    "\n",
    "    def forward(self, chunk_fft):\n",
    "        \n",
    "        mean = chunk_fft.mean(dim=(1, 2), keepdim=True)\n",
    "        std = chunk_fft.std(dim=(1, 2), keepdim=True)\n",
    "        chunk_fft = (chunk_fft - mean) / (std + 1e-5)\n",
    "        \n",
    "        y = self.split(chunk_fft)\n",
    "        y = self.sequence(y)\n",
    "        mask = self.mask(y)\n",
    "        \n",
    "        mask = mask * std + mean\n",
    "\n",
    "        return mask\n",
    "\n",
    "if visualize:\n",
    "    bsrnn = BSRNN().cuda()\n",
    "    bsrnn_y = bsrnn(chunk_stft.unsqueeze(0).cuda())\n",
    "    bsrnn.visualize(chunk_stft.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8d7a7-6e4a-4df8-b960-40ed48781336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(BSRNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4f830-f48c-4900-903b-593ab23c4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def from_spectogram():\n",
    "    # We are using GriffinLim to ensure the output size\n",
    "    transform_inv_spectogram = T.InverseSpectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length,\\\n",
    "                                                    window_fn=torch.hamming_window)\n",
    "\n",
    "    def inner(chunk_stft, visualize=False):\n",
    "        chunk = transform_inv_spectogram(chunk_stft)\n",
    "    \n",
    "        if visualize:\n",
    "            visualize_spectogram(chunk.detach().numpy(), chunk_stft)\n",
    "\n",
    "        return chunk\n",
    "    return inner\n",
    "\n",
    "if visualize:\n",
    "    print(bsrnn_y.shape)\n",
    "    masked_complex = chunk_stft * bsrnn_y[0].cpu()\n",
    "\n",
    "    chunk_y = from_spectogram()(masked_complex, visualize=True)\n",
    "    chunk_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b1032-7bc6-40f9-9745-854c3be034b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize and False:\n",
    "    chunk_x = de_normalize_waveform(sample_waveform_chunks[1], gain_factor, peak_gain_factor)\n",
    "    chunk_y = de_normalize_waveform(chunk_y, gain_factor, peak_gain_factor)\n",
    "\n",
    "    print(\"In:\")\n",
    "    show_idp_audio(chunk_x)\n",
    "    print(\"Out:\")\n",
    "    show_idp_audio(chunk_y.detach().numpy())\n",
    "    print(chunk_y.shape)\n",
    "    chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed9801-0fde-4110-a40f-95b20d787857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72ccde-384a-40da-b144-1206e527d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Everything Together! audio in, audio out!\n",
    "\n",
    "\n",
    "class MSSBandSplitRNN(nn.Module, Vis):\n",
    "    def __init__(self):\n",
    "        super(MSSBandSplitRNN, self).__init__()\n",
    "        self.to_spectogram = to_spectogram()\n",
    "        self.from_spectogram = from_spectogram()\n",
    "        self.bsrnn = BSRNN()\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\" Waveform in -> Waveform out :) \"\"\"\n",
    "        \n",
    "        # 1) normalize\n",
    "        # 2) split\n",
    "        # 3) feed to bsrnn\n",
    "        # 4) convert spectogram to audio\n",
    "        # 5) merge all splits\n",
    "        # 6) de-normalize\n",
    "        \n",
    "        normal_waveform, gain_factor, peak_gain_factor = normalize_waveform(waveform)\n",
    "        splits, padding_length = split(normal_waveform)\n",
    "        masked_splits = [() for _ in range(len(splits))]\n",
    "        for i, x_split in enumerate(splits):\n",
    "            split_stft = self.to_spectogram(x_split)\n",
    "            mask = self.bsrnn(split_stft.unsqueeze(0))[0]\n",
    "            \n",
    "            masked_complex = mask\n",
    "            \n",
    "            wave = self.from_spectogram(masked_complex)\n",
    "            masked_splits[i] = wave\n",
    "        \n",
    "        masked_waveform = merge(masked_splits, padding_length)\n",
    "        y = de_normalize_waveform(masked_waveform, gain_factor, peak_gain_factor)\n",
    "        return y\n",
    "\n",
    "if visualize:\n",
    "    torch.set_default_device('cuda')\n",
    "    with torch.no_grad():\n",
    "        model = MSSBandSplitRNN()\n",
    "        y = model(sample_waveform.cuda())\n",
    "\n",
    "\n",
    "    print(\"In:\")\n",
    "    show_idp_audio(sample_waveform)\n",
    "    print(\"Out:\")\n",
    "    show_idp_audio(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004c354-c93f-4d13-a326-3197b0b8a38f",
   "metadata": {},
   "source": [
    "# 2) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75190bcb-729e-4b92-ba27-f8b71421137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mae_stft_real = nn.L1Loss()\n",
    "        self.mae_stft_imag = nn.L1Loss()\n",
    "        self.mae_inv_stft  = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, pred_stft, target_stft,  pred_inv_stft, target_inv_stft):\n",
    "        loss_r = self.mae_stft_real(pred_stft.real, target_stft.real)\n",
    "        loss_i = self.mae_stft_imag(pred_stft.imag, target_stft.imag)\n",
    "        loss_t = self.mae_inv_stft(pred_inv_stft, target_inv_stft)\n",
    "        loss = loss_r + loss_i + loss_t\n",
    "        return loss\n",
    "\n",
    "def compute_usdr(pred, target, delta = 1e-7):\n",
    "    if pred.shape[0] < target.shape[0]:\n",
    "        padding = target.shape[0] - pred.shape[0]\n",
    "        pred = torch.nn.functional.pad(pred, (0, padding), \"constant\", 0)\n",
    "    num = torch.sum(torch.square(target))\n",
    "    den = torch.sum(torch.square(target - pred))\n",
    "    num += delta\n",
    "    den += delta\n",
    "    usdr = 10 * torch.log10(num / den)\n",
    "    return usdr.mean()\n",
    "\n",
    "if visualize and False:\n",
    "    torch.set_default_device('cpu')\n",
    "    target_wf = load_audio(\"drums.wav\")\n",
    "    normal_target_wf, gain_factor, peak_gain_factor = normalize_waveform(target_wf)\n",
    "    target_wf_chunks, padding_length = split(normal_target_wf)\n",
    "    stft = to_spectogram()\n",
    "    inv_stft = from_spectogram()\n",
    "    print(\"target:\")\n",
    "    show_idp_audio(target_wf_chunks[1])\n",
    "    target_stft = stft(target_wf_chunks[1], visualize=True, title=\"Target\")\n",
    "    print(\"input:\")\n",
    "    show_idp_audio(sample_waveform_chunks[1])\n",
    "    sample_stft = stft(sample_waveform_chunks[1], visualize=True, title=\"Input\")\n",
    "\n",
    "    # x * mask = target_stft / x\n",
    "    # f(x) = stft - \n",
    "\n",
    "    mask_stft = target_stft / sample_stft\n",
    "    visualize_spectogram(inv_stft(mask_stft), mask_stft, title=\"Mask stft\")\n",
    "    print(\"Mask:\")\n",
    "    show_idp_audio(inv_stft(mask_stft))\n",
    "    target_y = sample_stft * mask_stft\n",
    "    print(\"Target_y: \")\n",
    "    show_idp_audio(inv_stft(target_y))\n",
    "    visualize_spectogram(inv_stft(target_y), target_y, title=\"Target_y stft calc\")\n",
    "\n",
    "\n",
    "    loss_fn = CustomLoss()\n",
    "    loss = loss_fn.forward(target_y, target_stft, inv_stft(target_y), inv_stft(target_stft))\n",
    "    print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fc97f-3d1c-4faa-8f6f-ec06f611d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, part=\"drums\", dir_=\"/home/sahand/BandSplit-RNN/musdb18hq/\", validation=False, portion=1):\n",
    "        super(Dataset, self).__init__()\n",
    "        path = dir_ + (\"test\" if validation else \"train\")\n",
    "        self.part = part\n",
    "        self.files = [entry for entry in os.scandir(path) if entry.is_dir()]\n",
    "        random.shuffle(self.files) # we are seeding the random gen manually so this is fine.\n",
    "        self.files = self.files[:int(len(self.files) * portion)]\n",
    "    \n",
    "    def iterator(self):\n",
    "        torch.set_default_device('cpu')\n",
    "        to_stft = to_spectogram()\n",
    "        for i in range(self.start_index, self.end_index):\n",
    "            d = self.files[i]\n",
    "            mixture = f\"{d.path}/mixture.wav\"\n",
    "            target = f\"{d.path}/{self.part}.wav\"\n",
    "\n",
    "            normal_mix, _, _ = normalize_waveform(load_audio(mixture))\n",
    "            normal_target, _, _ = normalize_waveform(load_audio(target))\n",
    "\n",
    "            normal_mix, _ = split(normal_mix)\n",
    "            normal_target, _ = split(normal_target)\n",
    "\n",
    "            for mix, target in zip(normal_mix, normal_target):\n",
    "                mix_stft = to_stft(mix)\n",
    "                target_stft = to_stft(target)\n",
    "                # Accumulate STFTs in the batch\n",
    "                yield (mix_stft, target_stft)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            total_files = len(self.files)\n",
    "            per_worker = int(math.ceil(total_files / float(worker_info.num_workers)))\n",
    "            self.start_index = worker_info.id * per_worker\n",
    "            self.end_index = min(self.start_index + per_worker, total_files)\n",
    "        else:\n",
    "            self.start_index = 0\n",
    "            self.end_index = len(self.files)\n",
    "\n",
    "        return iter(self.iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75952a2-8af0-465a-8db3-9259e55768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf93d8-3df8-4651-8d0f-23a4d9294ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning = True\n",
    "batch_size=250 if transfer_learning else 95\n",
    "clip_grad_norm=5\n",
    "portion=0.1\n",
    "# bass, drums, other, vocals\n",
    "part = \"bass\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(Dataset(validation=False, part=part, portion=portion), num_workers=1, \n",
    "                                          batch_size=batch_size, drop_last=True,\n",
    "                                          prefetch_factor=4,\n",
    "                                          persistent_workers=True)\n",
    "val_loader   = torch.utils.data.DataLoader(Dataset(validation=True, part=part), num_workers=1, \n",
    "                                          batch_size=batch_size, drop_last=True,\n",
    "                                          prefetch_factor=4,\n",
    "                                          persistent_workers=True)\n",
    "\n",
    "\n",
    "model = BSRNN().to('cuda')\n",
    "import datetime\n",
    "\n",
    "# ./train-logs/generic-splits-2023-12-22 17:01:33.861454/models/checkpoint_158.pt\n",
    "# ./train-logs/baseline-2023-12-21 14:05:22.129601/checkpoint_58.pt\n",
    "checkpoint_fp = './train-logs/generic-splits-2023-12-22 17:01:33.861454/models/checkpoint_158.pt'\n",
    "name = f\"baseline-drums-to-{part}-portion-{portion}\"\n",
    "prefix=f\"./train-logs/{name}-{datetime.datetime.now()}\"\n",
    "if not os.path.exists(name):\n",
    "    os.makedirs(name)\n",
    "print(f\"run name={prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f2310-9d39-4b67-aef8-dbe262bd1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912587e-ca08-4eb1-be90-52c7a20112b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint, Checkpoint, DiskSaver, global_step_from_engine\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar, TensorboardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db422f6-b9d6-4755-9fb1-3e1eff52ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "inv_stft_gpu = from_spectogram()\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    mix_stft, mask_stft = batch\n",
    "    mix_stft, mask_stft = mix_stft.to('cuda'), mask_stft.to('cuda')\n",
    "    y_mask = model(mix_stft)\n",
    "    inv_y_mask = inv_stft_gpu(y_mask)\n",
    "    inv_mask_stft = inv_stft_gpu(mask_stft)\n",
    "    loss = loss_fn(y_mask, mask_stft, inv_y_mask, inv_mask_stft)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    usdr_value = compute_usdr(inv_y_mask, inv_mask_stft).item()\n",
    "    return {'loss': loss.item(), 'usdr': usdr_value}\n",
    "\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "running_avg_usdr = RunningAverage(output_transform=lambda x: x['usdr'])\n",
    "running_avg_usdr.attach(trainer, 'running_avg_usdr')\n",
    "running_avg_loss = RunningAverage(output_transform=lambda x: x['loss'])\n",
    "running_avg_loss.attach(trainer, 'running_avg_loss')\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir=f\"{prefix}/tb\")\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED,\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda x: {\"running_avg_loss\": trainer.state.metrics['running_avg_loss'], \n",
    "                                \"running_avg_usdr\": trainer.state.metrics['running_avg_usdr']}\n",
    ")\n",
    "\n",
    "def eval_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mix_stft, mask_stft = batch\n",
    "        mix_stft, mask_stft = mix_stft.to('cuda'), mask_stft.to('cuda')\n",
    "        y_mask = model(mix_stft)\n",
    "        inv_y_mask = inv_stft_gpu(y_mask)\n",
    "        inv_mask_stft = inv_stft_gpu(mask_stft)\n",
    "        usdr_value = compute_usdr(inv_y_mask, inv_mask_stft).item()\n",
    "        return {'usdr': usdr_value}\n",
    "\n",
    "evaluator = Engine(eval_step)\n",
    "\n",
    "running_avg_usdr_eval = RunningAverage(output_transform=lambda x: x['usdr'])\n",
    "running_avg_usdr_eval.attach(evaluator, 'running_avg_usdr_eval')\n",
    "\n",
    "@evaluator.on(Events.COMPLETED)\n",
    "def log_results(engine):\n",
    "    usdr = engine.state.metrics['running_avg_usdr_eval']\n",
    "    print(f\"Test Results - Avg usdr: {usdr}\")\n",
    "\n",
    "bar = ProgressBar(persist=False)\n",
    "bar.attach(evaluator, metric_names=['running_avg_usdr_eval'])\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def train_epoch_completed(engine):\n",
    "    epoch = engine.state.epoch\n",
    "    evaluator.run(val_loader)\n",
    "    tb_logger.add_scalar(\"evaluation/running_avg_usdr_eval\", evaluator.state.metrics['running_avg_usdr_eval'], epoch)\n",
    "\n",
    "\n",
    "\n",
    "# Model Checkpointing\n",
    "to_save = {\n",
    "    'trainer': trainer,\n",
    "    'model': model,\n",
    "    'optimizer': optimizer,\n",
    "    'running_avg_usdr': running_avg_usdr,\n",
    "    'running_avg_loss': running_avg_loss\n",
    "}\n",
    "checkpoint_handler = Checkpoint(\n",
    "    to_save,\n",
    "    DiskSaver(f'{prefix}/models', create_dir=True),\n",
    "    n_saved=25,\n",
    "    global_step_transform=global_step_from_engine(trainer)\n",
    ")\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler)\n",
    "\n",
    "# Resume Training\n",
    "if os.path.isfile(checkpoint_fp):\n",
    "    checkpoint = torch.load(checkpoint_fp)\n",
    "    Checkpoint.load_objects(to_load=to_save, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d446fd-2217-4695-9973-5351c52c8218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "if transfer_learning:\n",
    "    print(f\"Params before freeze -> {count_parameters(model)}\")\n",
    "    \n",
    "    # Freeze first two modules. keep the mask estimation\n",
    "    model.split.requires_grad_(False)\n",
    "    model.sequence.requires_grad_(False)\n",
    "    model.mask.requires_grad_(True)\n",
    "    # we should reset some states\n",
    "    running_avg_usdr.reset()\n",
    "    running_avg_loss.reset()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    trainer.state.max_epochs = None\n",
    "    print(f\"Params after freeze -> {count_parameters(model)}\")\n",
    "\n",
    "# Progress Bar\n",
    "pbar = ProgressBar(persist=True)\n",
    "pbar.attach(trainer, metric_names=['running_avg_loss', 'running_avg_usdr'])\n",
    "# Run Training\n",
    "trainer.run(train_loader, max_epochs=158)\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b06388-8355-493e-a93f-a4997be11436",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Log\n",
    "\n",
    "- `./train-logs/generic-splits-2023-12-22 17:01:33.861454/models/checkpoint_158.pt`\n",
    "\n",
    "Test Results - Avg usdr: 4.249491467122149\n",
    "\n",
    "- `./train-logs/baseline-2023-12-21 14:05:22.129601/checkpoint_58.pt`\n",
    "\n",
    "Test Results - Avg usdr: 4.173307912659941\n",
    "\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-2023-12-24 17:58:36.366905`\n",
    "\n",
    "Test Results - Avg usdr: 2.9780268712515574\n",
    "Params before freeze -> 2000816\n",
    "Params after freeze -> 1120244\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-second-try-2023-12-26 16:31:35.352266`\n",
    "\n",
    "Test Results - Avg usdr: 2.0048017067065538\n",
    "\n",
    "- `./train-logs/generic-splits-drums-to-bass-second-try-2023-12-26 20:50:55.859633`\n",
    "\n",
    "Test Results - Avg usdr: 4.420815479078954\n",
    "\n",
    "\n",
    "- `generic-splits-drums-to-bass-2023-12-25 07:14:41.478144`\n",
    "\n",
    "Test Results - Avg usdr: 3.608849364656641\n",
    "\n",
    "- `baseline-drums-to-bass-2023-12-25 10:04:36.031798`\n",
    "\n",
    "Test Results - Avg usdr: 2.708634780307086\n",
    "\n",
    "- `./train-logs/baseline-drums-to-vocals-2023-12-25 15:54:05.556248`\n",
    "\n",
    "Test Results - Avg usdr: 1.1394744329232132\n",
    "\n",
    "- `./train-logs/generic-splits-drums-to-vocals-2023-12-25 19:51:08.768699`\n",
    "\n",
    "Test Results - Avg usdr: 1.2581019524776116\n",
    "\n",
    "- `./train-logs/generic-splits-drums-to-other-2023-12-26 07:46:18.239555`\n",
    "\n",
    "Test Results - Avg usdr: 1.8895061738323766\n",
    "\n",
    "- `./train-logs/baseline-drums-to-other-2023-12-26 11:57:19.625444`\n",
    "\n",
    "Test Results - Avg usdr: 1.4147968221544212\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Portion\n",
    "\n",
    "- `./train-logs/generic-splits-drums-to-bass-portion-0.5-2023-12-27 10:07:42.013160`\n",
    "\n",
    "Test Results - Avg usdr: 3.151524824326728\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-portion-0.25-2023-12-27 21:05:18.319931`\n",
    "\n",
    "Test Results - Avg usdr: 3.342012454606739\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-portion-0.1-2023-12-27 23:19:56.135366`\n",
    "\n",
    "Test Results - Avg usdr: 3.2630481005295606\n",
    "\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-portion-0.5-2023-12-27 12:40:41.043017`\n",
    "\n",
    "Test Results - Avg usdr: 3.354360255148444\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-portion-0.25-2023-12-27 15:30:13.215559`\n",
    "\n",
    "Test Results - Avg usdr: 3.4292806904258533\n",
    "\n",
    "- `./train-logs/baseline-drums-to-bass-portion-0.1-2023-12-27 17:41:47.994008`\n",
    "\n",
    "Test Results - Avg usdr: 2.4986592458767016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34477a-78c7-42c5-9e8a-1e7af36bd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('cuda')\n",
    "m = MSSBandSplitRNN()\n",
    "model.eval()\n",
    "m.bsrnn = model\n",
    "mixture_wav = load_audio(\"mixture.wav\")\n",
    "target_wav = load_audio(\"drums.wav\")\n",
    "with torch.no_grad():\n",
    "    y = m(mixture_wav.cuda())\n",
    "show_idp_audio(mixture_wav)\n",
    "show_idp_audio(y.cpu().detach().numpy())\n",
    "show_idp_audio(target_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeeb202-466b-4eaa-b751-4551e2fd47d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bsrnn]",
   "language": "python",
   "name": "conda-env-bsrnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
